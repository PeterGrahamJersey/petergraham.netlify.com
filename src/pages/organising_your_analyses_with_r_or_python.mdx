---
title: 'Organising Your Analyses in R or Python'
date: '18/05/2019'
---

As a data analyst, it's inevitable that you'll have to update an old analysis with more recent numbers, usually to a tight deadline. When I first joined 15gifts I often found this more challenging than the analyses themselves; coming back to a mess of scripts and trying to work out which bit produced which graph was time-consuming and frustrating.

In addition to working with new data, at some point, your analyses will need to seamlessly work on another computer. Perhaps your work is being reviewed by a colleague, or you've just got a new laptop and need to review some older work.

There's a lot of overlap in organising your analyses to be replicable and to work on other computers. After a year at 15gifts,  it seemed like a good time to collate the principles that I would have found helpful when I started. Almost all of these principles are second nature for our developers, but they can be easily forgotten by, or never taught to, data analysts!

...

To enable our analyses to be re-run in future, and on other computers, we aim for them to be self-contained and understandable (to someone else). But how do we achieve this?

### Self-Contained

#### Give each analysis a dedicated folder

Our first step in making any analysis self-contained is giving it a dedicated folder. Where possible, everything from initial queries to our final output lives in this folder and if necessary we can include a text file with links to any cloud-based output (e.g. Google Slides decks).

#### Use relative paths to files

It is easy to rely on full file paths in our code:

```r
read_csv('petergraham/work/analysis-1/data/kpis.csv')
```

Unfortunately,  these are incredibly fragile. Scripts containing them will break with any changes to folder structure, and will not work on another computer without editing.

Instead of the full file path, we can simplify our code and make it less fragile by using `r>.` to refer to the current working directory (`r>..`refers to the parent directory, which can also come in handy). If we set our working directory to the dedicated folder for this analysis, then we can replace the above example with:

```r
read_csv('./data/kpis.csv')
```

Provided the working directory is set before running the analysis, all our file references will work no matter where we move our folder or whose computer we access it on.

How you set the working directory varies slightly depending on how you run your code:

- **Terminal/Command Line:** `bash>cd petergraham/work/analysis-1`
- **R console:** `r>setwd('petergraham/work/analysis-1')`
- **RStudio:** either of the above would work, but I'd recommend making use of [Projects](https://medium.com/r/?url=https%3A%2F%2Fsupport.rstudio.com%2Fhc%2Fen-us%2Farticles%2F200526207-Using-Projects)
- **Python:** `py>os.chdir('petergraham/work/analysis-1')`
- **Jupyter/R Notebooks:** by default, the working directory is the folder that the notebook is saved in

#### Run things from scratch every so often

Testing is an important part of any code-based work and this is no different for keeping our analyses self-contained. Restarting R/Python every so often and running our code from scratch will ensure that we are not relying on a library, module or variable from outside of our current analysis.

[This article](https://medium.com/r/?url=https%3A%2F%2Fwww.tidyverse.org%2Farticles%2F2017%2F12%2Fworkflow-vs-script%2F) goes into more detail about keeping work self-contained and the frustrations of running code that isn't.

### Understandable

#### Find a folder structure that works and stick to it

Now that our analyses have dedicated folders, we can make them easier to understand by following a consistent structure. There are many suggestions online for possible folder structures, but they share common elements. A general structure might look like this:

```
analysis-1/
|-- master.R (or master.ipynb)
|-- data/
|-- output/
|-- functions/
'-- sql/
```

- **master.R** ties together the entire analysis. From this one file, we should be able to understand the approach used and to execute the entire analysis (or at least know how the steps needed to run it).
- **data/** contains raw data used in the analysis and is usually treated as read-only.
- **output/** stores anything generated by our code (usually figures and .csv's).  We can treat everything in this folder as disposable because running the analysis again will reproduce the output.
- **functions/** houses any other scripts or functions used in our analysis.
- **sql/** acts as a reference for any queries used in our analysis. It might not be needed to run the analysis, but is especially helpful when returning to the analysis after any kind of break.

The most effective folder setup will vary depending on the precise nature of someone's role and workflow, for example, querying a data warehouse directly from our code removes the need for a data folder. Here are some other examples of structures relying on the same underlying principles:

- [This article](https://medium.com/r/?url=https%3A%2F%2Fnicercode.github.io%2Fblog%2F2013-04-05-projects%2F) formed the basis of this folder structure. Aimed at scientific papers it includes additional folders for a formal write-up of the analysis.
- [This medium post](https://medium.com/outlier-bio-blog/a-quick-guide-to-organizing-data-science-projects-updated-for-2016-4cbb1e6dac71) suggests a more complex data versioning structure better suited for extended research in academia.
- [This site](https://medium.com/r/?url=https%3A%2F%2Flearn.r-journalism.com%2Fen%2Fpublishing%2Fworkflow%2Fr-projects%2F) aimed at data journalists adds a folder for markdown files and their html or pdf output.

#### Keep the main script simple, use functions and notebooks for complexity

The goal of the master script is to provide a single place to run and understand the analysis, so it needs to be as readable as possible. Using functions - with names describing what they do - allows us to move the detail of long or complex code out of our master script, making it easier to understand.

Instead of:

```r
# master.R
library(tidyverse)

raw_data <- read_csv('./data/kpis.csv')

prepared_data <- raw_data
names(prepared_data) <- str_to_lower(names(prepared_data))
prepared_data$conversion_rate <-
prepared_data$conversions / prepared_data$visitors
# any other data preparation...
```

we keep master.R simple by moving the data preparation to a function:

```r
# master.R
library(tidyverse)
source('./R/prepare_data.R')
raw_data <- read_csv('./data/kpis.csv')
prepared_data <- prepare_data(raw_data)
```

```r
# /R/prepare_data.R
prepare_data <- function(df) {
  prepared_data <- df
  names(prepared_data) <- str_to_lower(names(prepared_data))
  prepared_data$conversion_rate <- prepared_data$conversions / prepared_data$visitors
  # any other data preparation...
}
```

One approach is to work in the main script or notebook most of the time, but periodically take time to move detailed or repeated code into functions. Waiting until the end of the analysis to do so is generally less effective, as it can feel hard to justify this instead of moving on to the next task.

Aside from making the master file more readable, using functions prompts us to make our code more modular. Future analyses that require the same processing or calculation can be completed quicker and more reliably using our already written and tested functions.

#### Add comments to code

It takes less than a minute to add a sentence or two about why we took a certain approach or the inputs and outputs of a function, but doing so saves priceless time in future, e.g.

```r
# Converting kpi counts to rates
```

For more complex scripts, comments can also be used to create sections to help structure code:

```r
# Getting Data -------------------------
```

At this point, however, Notebooks (e.g. R Markdown or Jupyter) may offer a more effective way to combine structure, description and code.

#### Summary

To enable our analyses to be re-run in future, and on other computers, we aim for them to be self-contained and understandable. We achieve this by:

- Containing each analysis in its own folder that follows a standard structure
- Using relative file paths (`r>.`)
- Restarting R/Python to test that analyses are self-contained
- Using a main notebook/script to tie an entire analysis together, with the details moved into functions
- Commenting our code
